{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get bike data\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hold out the last 60 days of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class Deep(object):    \n",
    "    def __init__(self, sizes, actype='R'):\n",
    "            self.depth = len(sizes)-1\n",
    "            weights = []\n",
    "            biases = []\n",
    "            for m,n in zip(sizes[:-1],sizes[1:]):\n",
    "                weights.append(np.random.randn(m,n)/np.sqrt(m))\n",
    "                biases.append(np.zeros(n))\n",
    "            self.weights = weights\n",
    "            self.biases = biases\n",
    "            self.actype = actype\n",
    "        \n",
    "    def act(self,u,c=1):\n",
    "        if self.actype == 'R':\n",
    "            return np.maximum(u,0)\n",
    "        else:\n",
    "            return 1./(1.+np.exp(-c*u))\n",
    "        \n",
    "    def act_diff(self,u,c=1):\n",
    "        if self.actype == 'R':\n",
    "            return (u>0)\n",
    "        else:\n",
    "            return c*np.exp(-c*u)/(1+np.exp(-c*u))**2\n",
    "                \n",
    "    def loss(self, X , y=None, reg=0.0):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "        depth = self.depth\n",
    "        pas=[np.dot(X,weights[0])+biases[0]]\n",
    "        \n",
    "        #Pass into the next layers\n",
    "        for i in range(1,depth):\n",
    "            pas.append(np.dot(self.act(pas[-1]),weights[i])+biases[i])\n",
    "        \n",
    "        #Calculate the loss function\n",
    "        out = pas[-1]    \n",
    "        L = np.mean((out-y)**2)\n",
    "        \n",
    "        L += reg * sum([np.linalg.norm(weights[i])**2 for i in range(depth)])\n",
    "        weights_grad = []\n",
    "        biases_grad = []\n",
    "        \n",
    "        #NOTE: it is not exactly the gradient of the loss function\n",
    "        grad = 2*(out-y)\n",
    "\n",
    "        #Backprogation here\n",
    "        for i in range(depth-1,0,-1):\n",
    "            weights_grad.append(self.act(pas[i-1]).T.dot(grad))\n",
    "            biases_grad.append(sum(grad,0))\n",
    "            grad = grad.dot(weights[i].T)*self.act_diff(pas[i-1])\n",
    "            \n",
    "        #The last one is calculated a bit differently    \n",
    "        weights_grad.append(X.T.dot(grad))\n",
    "        biases_grad.append(sum(grad,0))\n",
    "        \n",
    "        #Reorder the grads\n",
    "        grads = {'weights': weights_grad[: : -1], 'biases': biases_grad[: : -1]}     \n",
    "\n",
    "        return L, grads\n",
    "    \n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "              reg=5e-6, epochs = 1000,\n",
    "              batch_size=200, verbose=False):\n",
    "        losses = {'train':[],'validation':[]}\n",
    "        depth = self.depth\n",
    "        data_size = len(X)\n",
    "        perm = np.arange(data_size)\n",
    "        iter_epoch = int(max(data_size/batch_size,1))\n",
    "        n_iter = epochs * iter_epoch\n",
    "        start =0 \n",
    "        epoch =1\n",
    "        for it in range(n_iter):\n",
    "            X_batch = X[perm[start:start+batch_size],:]\n",
    "            y_batch = y[perm[start:start+batch_size]]\n",
    "            L, grads = self.loss(X_batch,y_batch,reg)\n",
    "\n",
    "            for i in range(depth):\n",
    "                self.weights[i] -= learning_rate*grads['weights'][i]\n",
    "                self.biases[i] -= learning_rate*grads['biases'][i]\n",
    "        \n",
    "            if start + batch_size> data_size:\n",
    "                training_accuracy = np.mean((self.predict(X)-y)**2)\n",
    "                vali_accuracy = np.mean((self.predict(X_val) - y_val)**2)\n",
    "                sys.stdout.write('\\r Epoch: %d... Loss: %f ... training accuracy: %f...validation accuracy: %f ' %(epoch, L,training_accuracy,vali_accuracy))\n",
    "                losses['train'].append(training_accuracy)    \n",
    "                losses['validation'].append(vali_accuracy)\n",
    "\n",
    "                start =0\n",
    "                epoch += 1\n",
    "                np.random.shuffle(perm)\n",
    "            else:\n",
    "                start = start + batch_size\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def predict(self,X):\n",
    "        pas = np.dot(X,self.weights[0])+self.biases[0]\n",
    "        for i in range(1,self.depth):\n",
    "            pas = np.dot(self.act(pas),self.weights[i])+self.biases[i]\n",
    "        return pas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "X = train_features.values\n",
    "y = np.array(train_targets['cnt'].values,ndmin=2).T\n",
    "\n",
    "X_val = val_features.values\n",
    "y_val = np.array(val_targets['cnt'].values,ndmin=2).T\n",
    "\n",
    "input_size = X.shape[1]\n",
    "sizes=[input_size,40, 5, 2, 1]\n",
    "learning_rate=1e-3\n",
    "learning_rate_decay=1.\n",
    "reg=5e-6 \n",
    "#reg=0.\n",
    "epochs = 10\n",
    "batch_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 437... Loss: 0.026549 ... training accuracy: 0.029392...validation accuracy: 0.166970 "
     ]
    }
   ],
   "source": [
    "bike_deep= Deep(sizes,actype='R')\n",
    "L =bike_deep.train(X,y,X_val,y_val,learning_rate=learning_rate, reg = reg,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses =L\n",
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.ylim(ymax=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = bike_deep.predict(test_features.values)*std + mean\n",
    "\n",
    "ax.plot(predictions, label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.iloc[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
