{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on ANN and Backpropagation\n",
    "\n",
    "\n",
    "Let us recall the structure of a (full-connected) neural network. \n",
    "\n",
    "- It consists of $k$-layers ordered sequentially: $\\ell_0,\\cdots,\\ell_{k-1}$. Each layer $\\ell_i$, $i=1,\\cdots, k-1$ is connected to the previously layer $\\ell_{i-1}$. The first layer $\\ell_0$ is connected to the input layer $\\mathcal{d}$, which stores the input data. \n",
    "\n",
    "\n",
    "- The connections explain how the information flows inside the neural network. The data $x$ flow from $\\mathcal{d}$ to $\\ell_0$ and are transformed into $\\Im_0$; $\\Im_0$, flows from $\\ell_0$ to $\\ell_1$, is transformed into $\\Im_1$, etc. The transformation of the information from one layer to the next is determined by a set of weights $W^{(i)}$ and biases $b^{(i)}$, as follows:\n",
    "     $$\\Im_0 = x W^{0} + b^{0},$$\n",
    "and \n",
    " $$\\Im_i = a(\\Im_{i-1}) W^{i} + b^{i}, \\quad i = 1,\\dots k-1.$$\n",
    "Here, $a$ is an activation function (mostly commonly, sigmoid or relu).\n",
    "\n",
    "__Terminologies__ (my convention, not necessarily everyone's):\n",
    "\n",
    "- The last layer is called the __output__ layer. The information received at this layer, $\\Im_{k-1}$ is called the output of the neural networks.\n",
    "\n",
    "- The number $k$ is called the __depth__ of the neural network.\n",
    "\n",
    "__Machine learning using neural network__ is a two-step procedure:\n",
    "\n",
    "- __Training__: Find the correct value of the weights and biases (i.e., determine the correct transformations of the data-flow in the network). This step is normally cast into an optimization problem: minimize a loss function $L$. This loss function is determined by the output $\\Im_{k-1}$ and the __labels__ of the input data. The set of the input data and their labels is called training data set. \n",
    "\n",
    "_NOTE_: To train the network, one simultaneously feeds a number data points $x$ into the network. Each data point $x$ is a _row_ vector. The data set is then a matrix, denoted by $X$. The data-flow is still as explained above, except that $x$ is replace by $X$. \n",
    "\n",
    "- __Prediction__: Use the trained neural network to predict the labels of new input data. \n",
    "\n",
    "\n",
    "__Backpropagation__ is the backbone of the gradient-based optimization techniques for ANN. It is a method to _conveniently_ compute the gradients $\\nabla_{W^{i}}L$ and $\\nabla_{b^{i}} L$. It proceed as follows:\n",
    "\n",
    "\n",
    "\n",
    "- Compute $\\mathcal{V}_{k-1} := \\nabla_{\\Im_{k-1}} L$ (typically can be done by hand). \n",
    "\n",
    "- Then, for $i=k-1,\\dots,1$:\n",
    "$$\\nabla_{W^{i}} L = a(\\Im_{i-1}^T) \\cdot \\mathcal{V}_{i},~~ \\nabla_{b^{i}} L = \\sum_j \\mathcal{V}_{i}(j),~~ \\mathcal{V}_{i-1} = [\\mathcal{V}_{i} \\cdot (W^{i})^T] * a'(\\Im_{i-1}).$$\n",
    "Here, $\\sum \\mathcal{V_{i}}(j)$ is the sum of all rows of $\\mathcal{V}_{i}$. \n",
    "\n",
    "- For $i = 0$: \n",
    "$$\\nabla_{W^{0}} L = X^T \\cdot \\mathcal{V}_{0},\\quad \\nabla_{b^{0}} L = \\sum_j \\mathcal{V}_{0}(j).$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
